<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Kudos AI | Blog | The Complete Beginner’s Guide to Gradient Descent (Beginner-Friendly & Detailed)</title>
    <meta name="description"
          content="A carefully structured, beginner-friendly guide to gradient descent: intuition first, the math behind the update, safe learning rates, tiny numeric examples, clear visuals, then SGD, momentum, Adam and practical Python code—plus a 3Blue1Brown video.">
    <meta name="keywords"
          content="gradient descent beginner tutorial, learning rate explained, Lipschitz constant, stochastic gradient descent, momentum, Nesterov, Adam optimizer, Python code">

    <!-- Template CSS & icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css">
    <link rel="stylesheet" href="/styles.css"/>

    <!-- Google tag (gtag.js) -->
    <script>(function(){var analyticsId="G-VCL644R1SZ";try{var consent=localStorage.getItem("kudosCookieConsent");if(consent!=="accepted"){window["ga-disable-"+analyticsId]=true;}}catch(e){window["ga-disable-"+analyticsId]=true;}})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VCL644R1SZ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-VCL644R1SZ');
    </script>
    <link rel="icon" type="image/x-icon" href="/Kudos_AI_favicon.png">

    <!-- MathJax for equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
      /* keep your template look */
      .highlight{color:#007bff;font-weight:bold;}
      p{margin-bottom:1em;}
      .math-equation{text-align:center;margin:1em 0;font-family:"Courier New",monospace;}
      pre{background:#f8f9fa;border:1px solid #ddd;padding:10px;border-radius:5px;overflow-x:auto;}
      .video-container{position:relative;padding-bottom:56.25%;height:0;overflow:hidden;max-width:100%;background:#000;margin:0 auto;border-radius:12px;}
      .video-container iframe{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:12px;}
      .image-container{text-align:center;margin:20px 0;}
      .responsive-image{max-width:100%;height:auto;display:inline-block;border-radius:8px;}
      h3{text-align:center;}h4{text-align:left;}

      /* friendly callouts */
      .eli5{background:#f6ffed;border-left:4px solid #52c41a;padding:10px;border-radius:4px;margin:14px 0;}
      .note{background:#eef7ff;border-left:4px solid #007bff;padding:10px;border-radius:4px;margin:14px 0;}
      .tip {background:#fff7e6;border-left:4px solid #ffbf00;padding:10px;border-radius:4px;margin:14px 0;}
      .warn{background:#ffecec;border-left:4px solid #ff4d4f;padding:10px;border-radius:4px;margin:14px 0;}
      .checklist li{margin-bottom:.4em;}
      .mini{font-size:.95em;color:#444;}
      .toc a{color:#007bff;text-decoration:none;}
      .toc a:hover{text-decoration:underline;}
      .kbd{font-family:monospace;background:#f1f1f1;border:1px solid #ddd;border-radius:4px;padding:2px 6px;}
      .boxed{border:1px solid #007bff;border-radius:6px;padding:8px 12px;background:#eef7ff;}
      .example{background:#fafafa;border:1px dashed #ccc;border-radius:6px;padding:10px;margin:14px 0;}
      .table-mini td,.table-mini th{padding:.4rem .6rem;}
    </style>
  </head>

  <body>
    <!-- ---------- NAVBAR ---------- -->
    <nav class="navbar navbar-expand-lg navbar-light" style="background-color:white;">
      <a class="navbar-brand" href="/index.html"><img width="180" src="/Kudos_AI_logo_transparent.png" alt="Kudos AI Logo"></a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#myNavbarToggler10"
              aria-controls="myNavbarToggler10" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="myNavbarToggler10">
        <ul class="navbar-nav mx-auto">
          <li class="nav-item"><a class="nav-link" href="/index.html">Home</a></li>
          <li class="nav-item"><a class="nav-link" href="/Blog.html">Blog</a></li>
          <li class="nav-item"><a class="nav-link" href="/Projects.html">Projects</a></li>
          <li class="nav-item"><a class="nav-link" href="/About-me.html">About</a></li>
          <li class="nav-item"><a class="nav-link" href="/Get-in-Touch.html">Get in Touch</a></li>
        </ul>
        <ul class="navbar-nav sm-icons mr-0 d-flex justify-content-center">
          <li class="nav-item"><a class="nav-link" target="_blank" href="https://www.linkedin.com/in/hamoutni/"><i class="fa-brands fa-linkedin-in" style="margin-top:7px;"></i></a></li>
          <li class="nav-item"><a class="nav-link" target="_blank" href="https://www.kaggle.com/anashamoutni"><i class="fab fa-kaggle"></i></a></li>
          <li class="nav-item"><a class="nav-link" target="_blank" href="https://github.com/AnasHamoutni"><i class="fab fa-github" style="margin-top:7px;"></i></a></li>
          <li class="nav-item"><a class="nav-link" target="_blank" href="https://x.com/anashamoutni"><i class="fab fa-x-twitter" style="margin-top:7px;"></i></a></li>
        </ul>
      </div>
    </nav>

    <!-- ---------- HERO IMAGE + TITLE ---------- -->
    <div class="blogPost-div">
      <img class="imgPost" loading="lazy"
           src="Gradient-Descent.jpg"
           alt="Gradient Descent Illustration"/>
      <div class="blogPost-title-div">
        <h2 class="blogPost-title">The Complete Beginner’s Guide to Gradient Descent (Beginner-Friendly & Detailed)</h2>
      </div>

      <!-- ---------- MAIN ARTICLE BODY ---------- -->
      <div class="postBody-div">

<!-- ===== ABSTRACT ===== -->
<p><em>Gradient descent is the workhorse behind modern machine learning. In this guide we go slow and logical: first the goal and the idea of a <span class="highlight"><a href="https://en.wikipedia.org/wiki/Loss_function" target="_blank">loss</a></span>, then the gradient and the famous one-line update, then safe learning-rate choices with tiny numeric examples and visuals. Only after that do we introduce practical variants (mini-batches, momentum, Adam) with clear definitions and code you can run.</em></p>

<!-- ===== ELI5 ===== -->
<div class="eli5">
  <b>ELI5:</b> Imagine a marble on a hilly landscape. Wherever the marble is, the steepest uphill direction is the <em>gradient</em>. To go down, step a little in the exact opposite direction. Repeat until you reach the valley.
</div>

<!-- ===== TOC ===== -->
<div class="note toc">
  <strong>Contents</strong> —
  <a href="#goal">1) Objective (what we minimize)</a> ·
  <a href="#gradient">2) Gradient (what direction to move)</a> ·
  <a href="#update">3) Update rule (why the minus sign)</a> ·
  <a href="#rate">4) Learning rate (safe choices)</a> ·
  <a href="#1d">5) 1D example (feel it numerically)</a> ·
  <a href="#visual">6) Visual intuition (contours)</a> ·
  <a href="#scale">7) Scaling & conditioning (why standardize)</a> ·
  <a href="#sgd">8) Mini-batches & SGD (fast gradients)</a> ·
  <a href="#momentum">9) Momentum (smooth the path)</a> ·
  <a href="#adam">10) Adam (adaptive steps + momentum)</a> ·
  <a href="#logreg">11) Hands-on: logistic regression</a> ·
  <a href="#diagnose">12) Diagnose training curves</a> ·
  <a href="#theory">13) Theory (just enough)</a> ·
  <a href="#loop">14) A tidy training loop</a> ·
  <a href="#video">15) Video</a> ·
  <a href="#glossary">16) Glossary</a> ·
  <a href="#refs">17) References</a> ·
  <a href="#faq">18) FAQ</a>
</div>

<!-- ========== 1. GOAL ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="goal" style="color:blue;margin-top:35px;margin-bottom:30px;">1) Objective: what are we minimizing?</h3>
<center><hr width="50%" class="solid"></center>

<p><b>Plain English.</b> We want the model to make fewer mistakes. We turn “mistakes” into a number called the <span class="highlight"><a href="https://en.wikipedia.org/wiki/Loss_function" target="_blank">loss</a></span>. Lower is better.</p>

<div class="math-equation">
$$
\text{Find } \theta^\star=\arg\min_{\theta\in\mathbb{R}^d}\; \mathcal{L}(\theta)
$$
</div>

<ul class="checklist">
  <li><b>Regression:</b> Mean Squared Error (MSE) \(\; \frac{1}{n}\sum_i (y_i-\hat y_i)^2\).</li>
  <li><b>Classification:</b> Cross-Entropy (log loss).</li>
  <li><b>Language models:</b> also Cross-Entropy—predicting the next token.</li>
</ul>

<!-- ========== 2. GRADIENT ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="gradient" style="color:blue;margin-top:35px;margin-bottom:30px;">2) Gradient: which way is downhill?</h3>
<center><hr width="50%" class="solid"></center>

<p><b>Idea.</b> The gradient points to the direction of <em>steepest increase</em>. So we walk the <em>opposite</em> way to reduce the loss fast.</p>

<div class="math-equation">
$$
\nabla \mathcal{L}(\theta)
=\left[\frac{\partial\mathcal{L}}{\partial\theta_1},\dots,\frac{\partial\mathcal{L}}{\partial\theta_d}\right]^\top
$$
</div>

<div class="tip mini">
<b>Quick check.</b> If nudging a parameter up makes loss go up (positive slope), step it down.
</div>

<!-- ========== 3. UPDATE RULE ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="update" style="color:blue;margin-top:35px;margin-bottom:30px;">3) The update rule (and the famous minus sign)</h3>
<center><hr width="50%" class="solid"></center>

<p>At step \(t\), with learning rate \(\eta\), take a small step opposite the gradient:</p>

<div class="math-equation">
$$
\theta_{t+1}=\theta_t - \eta\,\nabla_\theta \mathcal{L}(\theta_t)
$$
</div>

<p class="mini"><b>Why minus?</b> Gradients point uphill; we want downhill.</p>

<!-- ========== 4. LEARNING RATE ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="rate" style="color:blue;margin-top:35px;margin-bottom:30px;">4) Learning rate: how big should the step be?</h3>
<center><hr width="50%" class="solid"></center>

<p><b>Heuristic that works:</b> start small, increase until the loss gets bouncy, then back off a bit.</p>

<div class="note mini">
<b>Safe rule (quadratics):</b> if \(\mathcal{L}\) is a nice bowl with curvature bounded by \(L\) (largest eigenvalue of the Hessian), then any \(0<\eta<\frac{2}{L}\) guarantees the loss decreases each step.
</div>

<ul class="checklist">
  <li><b>LR finder:</b> raise \(\eta\) exponentially during a short run; pick the highest \(\eta\) before loss spikes, then divide by 3–10.</li>
  <li><b>Schedulers:</b> warmup (start small), step decay, cosine decay—helpful once basics work.</li>
</ul>

<!-- ========== 5. 1D EXAMPLE ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="1d" style="color:blue;margin-top:35px;margin-bottom:30px;">5) Tiny 1D example (feel it numerically)</h3>
<center><hr width="50%" class="solid"></center>

<div class="example">
<b>Function:</b> \(f(w)=(w-3)^2\) with minimum at \(w^\star=3\). Gradient \(f'(w)=2(w-3)\).  
<b>Update:</b> \(w \leftarrow w - \eta \cdot 2(w-3)\).  
<b>Stability:</b> it converges if \(0<\eta<1\) because the curvature (here \(L=2\)) gives \(2/L=1\).
</div>

<pre><code class="language-python">import numpy as np
def f(w):    return (w-3)**2
def grad(w): return 2*(w-3)

w, eta = 10.0, 0.1
for step in range(12):
    w -= eta * grad(w)
    print(f"step {step:2d}: w={w:6.3f}  loss={f(w):7.4f}")</code></pre>

<p>Try \(\eta=1.2\) to see divergence—useful for intuition.</p>

<!-- ========== 6. VISUALS ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="visual" style="color:blue;margin-top:35px;margin-bottom:30px;">6) Visual intuition (contour map)</h3>
<center><hr width="50%" class="solid"></center>

<div class="image-container">
  <img src="contour-Gradient-Descent.png" alt="Contour plot with arrows opposite gradients leading to the minimum" class="responsive-image">
</div>
<p><b>Reading the map.</b> Lines are equal-loss contours; arrows point down (opposite gradient). In narrow valleys, steps can zig-zag—later we’ll fix this with momentum.</p>

<!-- ========== 7. SCALING ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="scale" style="color:blue;margin-top:35px;margin-bottom:30px;">7) Scaling & conditioning (why standardize features)</h3>
<center><hr width="50%" class="solid"></center>

<p>When features have very different scales (e.g., age vs income), the loss “bowl” is stretched—some directions are steep, others flat. Gradient descent then zig-zags and needs a tiny \(\eta\). Standardizing inputs (zero mean, unit variance) makes the bowl rounder and training smoother.</p>

<div class="tip mini">
<b>Checklist:</b> standardize inputs; shuffle data; initialize weights small; monitor both training and validation loss.
</div>

<!-- ========== 8. SGD (AFTER BASICS) ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="sgd" style="color:blue;margin-top:35px;margin-bottom:30px;">8) Mini-batches & Stochastic Gradient Descent (now that GD is clear)</h3>
<center><hr width="50%" class="solid"></center>

<p>Full-batch gradient uses all \(n\) examples every step—accurate but slow. <span class="highlight"><a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank">Stochastic GD</a></span> uses one example—fast but very noisy. <b>Mini-batch</b> (e.g., 64–256 samples) balances speed and stability.</p>

<div class="math-equation">
$$
g_t \;=\; \frac{1}{|B_t|}\sum_{i\in B_t}\nabla_\theta \ell(\theta; x_i,y_i),
\qquad
\theta_{t+1}=\theta_t-\eta\,g_t.
$$
</div>

<p class="mini">The noise in \(g_t\) can help escape saddles and poor local minima.</p>

<!-- ========== 9. MOMENTUM ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="momentum" style="color:blue;margin-top:35px;margin-bottom:30px;">9) Momentum: keep some of your previous direction</h3>
<center><hr width="50%" class="solid"></center>

<p><b>Intuition.</b> If you’ve been moving east, don’t instantly stop unless the slope forces you. Momentum damps zig-zags in narrow valleys.</p>

<div class="math-equation">
$$
v_{t+1}=\gamma v_t + \eta\,\nabla \mathcal{L}(\theta_t),\qquad
\theta_{t+1}=\theta_t - v_{t+1},\quad \gamma\approx0.9.
$$
</div>

<p class="mini">Nesterov momentum peeks ahead a little before taking a step—often slightly better in practice.</p>

<!-- ========== 10. ADAM ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="adam" style="color:blue;margin-top:35px;margin-bottom:30px;">10) Adam: momentum + adaptive per-parameter step sizes</h3>
<center><hr width="50%" class="solid"></center>

<p><b>Idea.</b> Keep a moving average of gradients (like momentum) and also of squared gradients to scale each parameter’s step. Parameters with noisy gradients get smaller steps.</p>

<div class="math-equation">
$$
\begin{aligned}
m_t&=\beta_1 m_{t-1}+(1-\beta_1)g_t,\\
v_t&=\beta_2 v_{t-1}+(1-\beta_2)g_t^2,\\
\hat m_t&=m_t/(1-\beta_1^t),\ \hat v_t=v_t/(1-\beta_2^t),\\
\theta_{t+1}&=\theta_t-\eta\,\frac{\hat m_t}{\sqrt{\hat v_t}+\epsilon},
\end{aligned}
$$
</div>

<p class="mini"><b>Popular defaults:</b> \(\beta_1=0.9,\;\beta_2=0.999,\;\epsilon=10^{-8},\;\eta=10^{-3}\).</p>

<!-- ========== 11. HANDS-ON LOGISTIC REGRESSION ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="logreg" style="color:blue;margin-top:35px;margin-bottom:30px;">11) Hands-on: logistic regression trained with gradient descent</h3>
<center><hr width="50%" class="solid"></center>

<pre><code class="language-python">import numpy as np
from sklearn.datasets import make_classification
from sklearn.preprocessing import StandardScaler

# 1) Toy data
X, y = make_classification(n_samples=400, n_features=2, n_redundant=0,
                           n_clusters_per_class=1, random_state=0)
scaler = StandardScaler()
X = scaler.fit_transform(X)
X = np.c_[np.ones(len(X)), X]  # add bias column of ones

# 2) Model & helpers
w = np.zeros(X.shape[1])
def sigmoid(z): return 1/(1+np.exp(-z))
def loss(w):
    z = X @ w
    p = sigmoid(z)
    eps = 1e-9
    return -np.mean(y*np.log(p+eps) + (1-y)*np.log(1-p+eps))
def grad(w):
    z = X @ w
    p = sigmoid(z)
    return X.T @ (p - y) / len(y)

# 3) Train
eta = 0.2
for step in range(800):
    w -= eta*grad(w)
    if step % 100 == 0:
        print(f"step {step:4d}  loss={loss(w):.4f}")
print("weights:", w.round(3))</code></pre>

<p><b>What you should see.</b> Loss drops quickly then stabilizes; if it oscillates, lower \(\eta\); if it barely moves, increase \(\eta\) a bit.</p>

<!-- ========== 12. DIAGNOSE ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="diagnose" style="color:blue;margin-top:35px;margin-bottom:30px;">12) Diagnosing training curves (simple rules)</h3>
<center><hr width="50%" class="solid"></center>

<ul class="checklist">
  <li><b>Loss barely moves:</b> raise \(\eta\) (×2), standardize inputs, or try momentum \(0.9\).</li>
  <li><b>Loss explodes/NaN:</b> lower \(\eta\); clip gradients; check for bad labels or log(0).</li>
  <li><b>Loss zig-zags:</b> lower \(\eta\) slightly or add momentum; consider Adam.</li>
  <li><b>Train ↓, Val ↑:</b> overfitting—use regularization (e.g., L2), reduce capacity, or early stopping.</li>
</ul>

<!-- ========== 13. THEORY ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="theory" style="color:blue;margin-top:35px;margin-bottom:30px;">13) Just enough theory (why steps reduce loss)</h3>
<center><hr width="50%" class="solid"></center>

<p>If the gradient is <span class="highlight"><a href="https://en.wikipedia.org/wiki/Lipschitz_continuity" target="_blank">Lipschitz-continuous</a></span> with constant \(L\) and \(0<\eta<2/L\), each step decreases the loss unless you’re at a flat optimum:</p>

<div class="math-equation">
$$
\mathcal{L}(\theta_{t+1}) \le
\mathcal{L}(\theta_t) - \frac{\eta}{2}\Bigl(1-\frac{\eta L}{2}\Bigr)\|\nabla \mathcal{L}(\theta_t)\|^2.
$$
</div>

<p>For a 1D quadratic \(f(w)=\tfrac{a}{2}(w-w^\star)^2\), the update is \(w_{t+1}-w^\star=(1-\eta a)(w_t-w^\star)\). Convergence requires \(|1-\eta a|<1\Rightarrow 0<\eta<2/a\).</p>

<!-- ========== 14. TRAINING LOOP ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="loop" style="color:blue;margin-top:35px;margin-bottom:30px;">14) A tidy training loop (pseudo)</h3>
<center><hr width="50%" class="solid"></center>

<pre><code class="language-python"># Pseudo-code for a robust loop
init θ
optimizer = Adam(η=1e-3)         # or SGD+momentum once you’re comfortable
for epoch in range(E):
    for (x, y) in dataloader:    # mini-batches
        y_hat = model(x, θ)
        L     = loss(y_hat, y)
        g     = ∇θ L             # backprop computes this in frameworks
        θ     = optimizer.update(θ, g)
    # evaluate on validation set
    # adjust η with a scheduler if needed
    # early stop if validation loss worsens for K epochs</code></pre>

<div class="tip mini">
<b>Defaults that often work:</b> batch 64–256, Adam with \(\eta=10^{-3}\), weight decay (L2) if overfitting, warmup for a few epochs, then cosine or step LR schedule.
</div>

<!-- ========== 15. VIDEO ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="video" style="color:blue;margin-top:35px;margin-bottom:30px;">15) Video: 3Blue1Brown on gradient descent</h3>
<center><hr width="50%" class="solid"></center>

<div class="video-container" style="width:95%;margin:0 auto;">
  <iframe src="https://www.youtube.com/embed/IHZwWFHWa-w"
          title="Gradient descent, how neural networks learn | 3Blue1Brown"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen></iframe>
</div>
<br/>

<!-- ========== 16. GLOSSARY ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="glossary" style="color:blue;margin-top:35px;margin-bottom:30px;">16) Glossary (jargon → plain English)</h3>
<center><hr width="50%" class="solid"></center>

<ul>
  <li><b>Loss function</b> — number measuring how wrong the model is.</li>
  <li><b>Gradient</b> — direction of steepest loss increase.</li>
  <li><b>Learning rate (η)</b> — step size each update.</li>
  <li><b>Mini-batch</b> — small subset used to estimate gradient quickly.</li>
  <li><b>Momentum</b> — moving average of gradients to smooth updates.</li>
  <li><b>Adam</b> — optimizer combining momentum + adaptive steps.</li>
  <li><b>Conditioning</b> — how stretched the loss bowl is; bad conditioning slows GD.</li>
  <li><b>Scheduler</b> — planned changes to \(\eta\) over training.</li>
</ul>

<!-- ========== 17. REFERENCES ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="refs" style="color:blue;margin-top:35px;margin-bottom:30px;">17) References & Further Reading</h3>
<center><hr width="50%" class="solid"></center>

<ol>
  <li>3Blue1Brown. “Gradient descent, how neural networks learn.” (YouTube).</li>
  <li>Ruder, S. “An Overview of Gradient Descent Optimization Algorithms.” arXiv:1609.04747.</li>
  <li>Goodfellow, Bengio, Courville. <em>Deep Learning</em>, Ch. 8 (Optimization).</li>
  <li>Wikipedia: <a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank">Gradient Descent</a>, <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank">Stochastic GD</a>, <a href="https://en.wikipedia.org/wiki/Adam_(optimizer)" target="_blank">Adam</a>.</li>
</ol>

<!-- ========== 18. FAQ ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="faq" style="color:blue;margin-top:35px;margin-bottom:30px;">18) FAQ</h3>
<center><hr width="50%" class="solid"></center>

<h4>Q1. How do I pick a learning rate without guessing?</h4>
<p>Use an LR finder: start tiny, increase \(\eta\) exponentially, plot loss vs \(\eta\), pick the largest value before the curve turns up, then divide by 3–10.</p>

<h4>Q2. Should I always use Adam?</h4>
<p>Adam is great for noisy/sparse gradients and fast starts. For some vision tasks, well-tuned SGD+momentum can match or beat final accuracy. Try Adam first, then compare.</p>

<h4>Q3. My loss is NaN—why?</h4>
<p>Usually too-large \(\eta\) or invalid math (e.g., log(0)). Lower \(\eta\), standardize inputs, add gradient clipping (e.g., clip global \(\ell_2\) norm to 1), add small eps where needed.</p>

<h4>Q4. Does batch size matter?</h4>
<p>Yes: larger batches give stabler gradients but may need warmup and schedule tweaks; smaller batches add helpful noise and can generalize well. Start with 64–256.</p>

      </div> <!-- /postBody-div -->
    </div> <!-- /blogPost-div -->

    <!-- ---------- FOOTER ---------- -->
    <footer class="footer">
      <div class="waves">
        <div class="wave" id="wave1"></div>
        <div class="wave" id="wave2"></div>
        <div class="wave" id="wave3"></div>
      </div>
      <div class="footer-elem-container">
        <ul class="social-icon">
          <li class="social-icon__item"><a class="social-icon__link" href="https://www.linkedin.com/in/hamoutni/" target="_blank"><ion-icon name="logo-linkedIn"></ion-icon></a></li>
          <li class="social-icon__item"><a class="social-icon__link" href="https://www.kaggle.com/anashamoutni" target="_blank"><i class="fab fa-kaggle foote"></i></a></li>
          <li class="social-icon__item"><a class="social-icon__link" href="https://github.com/AnasHamoutni" target="_blank"><ion-icon name="logo-github"></ion-icon></a></li>
          <li class="social-icon__item"><a class="social-icon__link" href="https://x.com/anashamoutni" target="_blank">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 462.799" width="32" height="32" style="vertical-align:baseline;"><path fill="#fff" fill-rule="nonzero" d="M403.229 0h78.506L310.219 196.04 512 462.799H354.002L230.261 301.007 88.669 462.799h-78.56l183.455-209.683L0 0h161.999l111.856 147.88L403.229 0zm-27.556 415.805h43.505L138.363 44.527h-46.68l283.99 371.278z"/></svg>
          </a></li>
        </ul>
    <ul class="menu">
      <li class="menu__item"><a class="menu__link" href="/index.html">Home</a></li>
      <li class="menu__item"><a class="menu__link" href="/Blog.html">Blog</a></li>
          <li class="menu__item"><a class="menu__link" href="/Projects.html">Projects</a></li>
          <li class="menu__item"><a class="menu__link" href="/About-me.html">About</a></li>
      <li class="menu__item"><a class="menu__link" href="/Get-in-Touch.html">Get in Touch</a></li>
      <li class="menu__item"><a class="menu__link" href="/Privacy-Policy.html">Privacy Policy</a></li>
      <li class="menu__item"><a class="menu__link" href="/Terms-of-Service.html">Terms of Service</a></li>
      <li class="menu__item"><a class="menu__link" href="/Cookie-Policy.html">Cookie Policy</a></li>
    </ul>
<div class="p-foot-div">
    <p>&copy;<span class="js-copyright-year">2025</span> Kudos AI • by Anas HAMOUTNI • All Rights Reserved</p>
</div>
</div>
    </footer>

    <!-- ---------- SCRIPTS ---------- -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script>
    <script nomodule src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.js"></script>
    <script src='https://kit.fontawesome.com/d25a67fa19.js' crossorigin='anonymous' defer></script>
    <script src="/index.js" defer></script>
  </body>
</html>
