<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Kudos AI | Blog | How Self-Attention Works — Visually Explained</title>
    <meta name="description"
          content="A careful, visual, beginner-friendly deep dive into self-attention: queries, keys, values, scaled dot-product attention, masking, multi-head, positional encoding, encoder vs decoder vs cross-attention, complexity, plus NumPy/PyTorch code, tiny numeric examples, and a Karpathy video.">
    <meta name="keywords"
          content="self-attention, scaled dot-product attention, multi-head attention, positional encoding, transformer, attention is all you need, PyTorch MultiheadAttention, numpy attention implementation, causal mask, encoder-decoder">

    <!-- Template CSS & icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css">
    <link rel="stylesheet" href="/styles.css"/>

    <!-- Google tag (gtag.js) -->
    <script>(function(){var analyticsId="G-VCL644R1SZ";try{var consent=localStorage.getItem("kudosCookieConsent");if(consent!=="accepted"){window["ga-disable-"+analyticsId]=true;}}catch(e){window["ga-disable-"+analyticsId]=true;}})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VCL644R1SZ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){ dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'G-VCL644R1SZ');
    </script>
    <link rel="icon" type="image/x-icon" href="/Kudos_AI_favicon.png">

    <!-- MathJax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
      /* keep your template look + gentle helpers */
      .highlight{color:#007bff;font-weight:bold;}
      p{margin-bottom:1em;}
      .math-equation{text-align:center;margin:1em 0;font-family:"Courier New",monospace;}
      pre{background:#f8f9fa;border:1px solid #ddd;padding:10px;border-radius:5px;overflow-x:auto;}
      .video-container{position:relative;padding-bottom:56.25%;height:0;overflow:hidden;max-width:100%;background:#000;margin:0 auto;border-radius:12px;}
      .video-container iframe{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:12px;}
      .image-container{text-align:center;margin:20px 0;}
      .responsive-image{max-width:100%;height:auto;display:inline-block;border-radius:8px;}
      h3{text-align:center;}h4{text-align:left;}
      .eli5{background:#f6ffed;border-left:4px solid #52c41a;padding:10px;border-radius:4px;margin:14px 0;}
      .note{background:#eef7ff;border-left:4px solid #007bff;padding:10px;border-radius:4px;margin:14px 0;}
      .tip {background:#fff7e6;border-left:4px solid #ffbf00;padding:10px;border-radius:4px;margin:14px 0;}
      .warn{background:#ffecec;border-left:4px solid #ff4d4f;padding:10px;border-radius:4px;margin:14px 0;}
      .checklist li{margin-bottom:.4em;}
      .mini{font-size:.95em;color:#444;}
      .toc a{color:#007bff;text-decoration:none;}
      .toc a:hover{text-decoration:underline;}
      .kbd{font-family:monospace;background:#f1f1f1;border:1px solid #ddd;border-radius:4px;padding:2px 6px;}
      .boxed{border:1px solid #007bff;border-radius:6px;padding:8px 12px;background:#eef7ff;}
      .example{background:#fafafa;border:1px dashed #ccc;border-radius:6px;padding:10px;margin:14px 0;}
      .table-mini td,.table-mini th{padding:.45rem .6rem;}
      .shape{font-family:monospace;color:#333;background:#f6f8fa;border:1px solid #eaecef;padding:4px 8px;border-radius:4px;}
    </style>
  </head>

  <body>
    <!-- ---------- NAVBAR ---------- -->
    <nav class="navbar navbar-expand-lg navbar-light" style="background-color:white;">
      <a class="navbar-brand" href="/index.html"><img width="180" src="/Kudos_AI_logo_transparent.png" alt="Kudos AI Logo"></a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#myNavbarToggler10"
              aria-controls="myNavbarToggler10" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="myNavbarToggler10">
        <ul class="navbar-nav mx-auto">
          <li class="nav-item"><a class="nav-link" href="/index.html">Home</a></li>
          <li class="nav-item"><a class="nav-link" href="/Projects.html">Projects</a></li>
          <li class="nav-item"><a class="nav-link" href="/About-me.html">About</a></li>
          <li class="nav-item"><a class="nav-link" href="/Blog.html">Blog</a></li>
          <li class="nav-item"><a class="nav-link" href="/Get-in-Touch.html">Get in Touch</a></li>
        </ul>
        <ul class="navbar-nav sm-icons mr-0 d-flex justify-content-center">
          <li class="nav-item"><a class="nav-link" target="_blank" href="https://www.linkedin.com/in/hamoutni/"><i class="fa-brands fa-linkedin-in" style="margin-top:7px;"></i></a></li>
          <li class="nav-item"><a class="nav-link" target="_blank" href="https://www.kaggle.com/anashamoutni"><i class="fab fa-kaggle"></i></a></li>
          <li class="nav-item"><a class="nav-link" target="_blank" href="https://github.com/AnasHamoutni"><i class="fab fa-github" style="margin-top:7px;"></i></a></li>
          <li class="nav-item"><a class="nav-link" target="_blank" href="https://x.com/anashamoutni"><i class="fab fa-x-twitter" style="margin-top:7px;"></i></a></li>
        </ul>
      </div>
    </nav>

    <!-- ---------- HERO IMAGE + TITLE ---------- -->
    <div class="blogPost-div">
      <img class="imgPost" loading="lazy"
           src="Self-Attention-Visual-Diagram.png"
           alt="Self-Attention Visual Diagram"/>
      <div class="blogPost-title-div">
        <h2 class="blogPost-title">How Self-Attention Works — Visually Explained</h2>
      </div>

      <!-- ---------- MAIN ARTICLE BODY ---------- -->
      <div class="postBody-div">

<!-- ===== ABSTRACT ===== -->
<p><em><span class="highlight"><a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)" target="_blank">Self-attention</a></span> is the Transformer’s secret: each token decides which other tokens matter, and by how much. In this gentle guide, we build your intuition first, then show the math, shapes, and a tiny numerical walk-through. We’ll cover scaled dot-product attention, masking, <span class="highlight"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" target="_blank">multi-head attention</a></span>, positional encoding, encoder vs decoder vs cross-attention, complexity, and practical PyTorch/NumPy code you can run.</em></p>

<!-- ===== ELI5 ===== -->
<div class="eli5">
  <b>ELI5:</b> Imagine a group discussion where every word can “look at” the other words and decide who to listen to. The louder a word speaks (higher attention), the more it influences the final meaning of the sentence.
</div>

<!-- ===== TOC ===== -->
<div class="note toc">
  <strong>Contents</strong> —
  <a href="#intuition">1) Intuition</a> ·
  <a href="#qkv">2) Queries, Keys, Values (Q/K/V)</a> ·
  <a href="#scaled">3) Scaled Dot-Product Attention</a> ·
  <a href="#tiny">4) Tiny numeric example</a> ·
  <a href="#mask">5) Masking (causal & padding)</a> ·
  <a href="#multihead">6) Multi-Head Attention</a> ·
  <a href="#posenc">7) Positional Encoding</a> ·
  <a href="#blocks">8) Encoder, Decoder & Cross-Attention</a> ·
  <a href="#complexity">9) Complexity & efficiency</a> ·
  <a href="#numpy">10) NumPy implementation</a> ·
  <a href="#pytorch">11) PyTorch implementation</a> ·
  <a href="#visuals">12) Visualizing attention</a> ·
  <a href="#glossary">13) Glossary</a> ·
  <a href="#refs">14) References</a> ·
  <a href="#video">15) Video</a> ·
  <a href="#faq">16) FAQ</a>
</div>

<!-- ========== 1. INTUITION ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="intuition" style="color:blue;margin-top:35px;margin-bottom:30px;">1) Intuition: why attention?</h3>
<center><hr width="50%" class="solid"></center>

<p>In language, each word’s meaning depends on context. In “<em>The bank will not <b>loan</b> money</em>,” the word “bank” relates to finance; in “<em>The boat reached the <b>bank</b></em>,” it relates to a river. Self-attention lets each token <em>ask</em> the others, “Are you relevant to me?” and then blend information accordingly.</p>

<div class="image-container">
  <img src="contextual-relevance.png" alt="Contextual relevance arrows between words" class="responsive-image">
</div>

<p>Unlike RNNs (sequential) or CNNs (local), attention compares <em>every</em> token with every other token in parallel, which is powerful—but also quadratic in sequence length.</p>

<!-- ========== 2. QKV ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="qkv" style="color:blue;margin-top:35px;margin-bottom:30px;">2) Queries, Keys, Values (Q/K/V)</h3>
<center><hr width="50%" class="solid"></center>

<p>Start with token embeddings \(X\in\mathbb{R}^{n\times d_\text{model}}\) (n = tokens). We learn three projections:</p>

<div class="math-equation">
$$
Q = XW_Q,\quad K = XW_K,\quad V = XW_V,
$$
</div>

<p>with \(W_Q,W_K,W_V\in\mathbb{R}^{d_\text{model}\times d_k}\) (often \(d_k=d_v=d_\text{model}/h\) for h heads). Intuition:</p>
<ul class="checklist">
  <li><b>Query</b>: what a token is <em>looking for</em>.</li>
  <li><b>Key</b>: what a token <em>offers</em>.</li>
  <li><b>Value</b>: the actual information to pass along.</li>
</ul>

<p class="mini"><span class="shape">Shapes</span> — If \(X\) is <code>[n, d_model]</code>, and we use one head with \(d_k\), then \(Q,K,V\) are <code>[n, d_k]</code>.</p>

<!-- ========== 3. SCALED DOT-PRODUCT ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="scaled" style="color:blue;margin-top:35px;margin-bottom:30px;">3) Scaled Dot-Product Attention</h3>
<center><hr width="50%" class="solid"></center>

<p>Scores are dot products between queries and keys. We scale by \(\sqrt{d_k}\) to keep gradients stable and apply <span class="highlight"><a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank">softmax</a></span> row-wise:</p>

<div class="math-equation">
$$
\text{Attn}(Q,K,V)=\underbrace{\text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)}_{\text{attention weights }A}\;V.
$$
</div>

<p>The matrix \(A\in\mathbb{R}^{n\times n}\) holds how strongly each token attends to each other token. Each row sums to 1.</p>

<!-- ========== 4. TINY NUMERIC EXAMPLE ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="tiny" style="color:blue;margin-top:35px;margin-bottom:30px;">4) Tiny numeric example (3 tokens, 2-D head)</h3>
<center><hr width="50%" class="solid"></center>

<div class="example">
Let \(Q,K,V\in\mathbb{R}^{3\times 2}\). Suppose
\[
Q=\begin{bmatrix}1&0\\0&1\\1&1\end{bmatrix},\;
K=\begin{bmatrix}1&1\\1&0\\0&1\end{bmatrix},\;
V=\begin{bmatrix}2&0\\0&2\\1&1\end{bmatrix}.
\]
Compute \(S=QK^\top\) (3×3):
\[
S=\begin{bmatrix}
1\cdot1+0\cdot1 & 1\cdot1+0\cdot0 & 1\cdot0+0\cdot1\\
0\cdot1+1\cdot1 & 0\cdot1+1\cdot0 & 0\cdot0+1\cdot1\\
1\cdot1+1\cdot1 & 1\cdot1+1\cdot0 & 1\cdot0+1\cdot1
\end{bmatrix}
=
\begin{bmatrix}
1&1&0\\
1&0&1\\
2&1&1
\end{bmatrix}.
\]
Scale by \(\sqrt{d_k}=\sqrt{2}\) and softmax each row to get \(A\). For row 1: softmax\((\tfrac{1}{\sqrt2},\tfrac{1}{\sqrt2},0)\) ≈ (0.401,0.401,0.198).  
Then output = \(A V\) → each token becomes a weighted mix of the rows of \(V\).
</div>

<p>This small arithmetic is worth doing once by hand; it makes the abstraction “click.”</p>

<!-- ========== 5. MASKING ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="mask" style="color:blue;margin-top:35px;margin-bottom:30px;">5) Masking: causal (decoder) & padding (batching)</h3>
<center><hr width="50%" class="solid"></center>

<p><b>Causal mask</b> (used in decoders / autoregressive models like GPT): token \(t\) cannot look ahead to \(t+1,\dots\). We add \(-\infty\) to forbidden logits before softmax.</p>

<div class="math-equation">
$$
A=\text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}+\text{mask}\right),
\quad \text{mask}_{ij}=\begin{cases}0 & j\le i\\ -\infty & j>i.\end{cases}
$$
</div>

<p><b>Padding mask</b>: When batching sequences of different lengths, pad with zeros and mask those positions so they receive and send no attention.</p>

<!-- ========== 6. MULTI-HEAD ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="multihead" style="color:blue;margin-top:35px;margin-bottom:30px;">6) Multi-Head Attention (why more than one head?)</h3>
<center><hr width="50%" class="solid"></center>

<p>One head learns one notion of “relevance.” Multiple heads let the model attend to different patterns (syntax, coreference, long-range dependencies) simultaneously. Each head has its own \(W_Q,W_K,W_V\), smaller per-head dimension \(d_k\), and outputs are concatenated then projected:</p>

<div class="math-equation">
$$
\text{MHA}(X) = \text{Concat}(\text{head}_1,\dots,\text{head}_h)\,W_O.
$$
</div>

<div class="image-container">
  <img src="Multi-Head-Attention.png" alt="Multi-head attention: split, attend, concat, project" class="responsive-image">
</div>

<p class="mini"><span class="shape">Shapes</span> — With <code>h</code> heads and model dim <code>d_model</code>, we often set <code>d_k = d_model / h</code>. Each head: <code>[n, d_k]</code> → concat: <code>[n, h*d_k]=[n, d_model]</code>.</p>

<!-- ========== 7. POSITIONAL ENCODING ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="posenc" style="color:blue;margin-top:35px;margin-bottom:30px;">7) Positional Encoding (order matters!)</h3>
<center><hr width="50%" class="solid"></center>

<p>Self-attention has no sense of order by itself. We add <span class="highlight"><a href="https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6" target="_blank">positional encodings</a></span> to token embeddings so the model knows token positions.</p>

<div class="math-equation">
$$
\text{PE}_{(pos,2i)} = \sin\!\left(\frac{pos}{10000^{2i/d_\text{model}}}\right),\quad
\text{PE}_{(pos,2i+1)} = \cos\!\left(\frac{pos}{10000^{2i/d_\text{model}}}\right).
$$
</div>

<p>These sinusoids let the model infer relative positions. Many variants exist (learned, rotary, ALiBi, etc.), but sinusoidal remains a clear starting point.</p>

<div class="image-container">
  <img src="positional_encoding.png" alt="Sinusoidal positional encoding stripes" class="responsive-image">
</div>

<!-- ========== 8. BLOCKS ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="blocks" style="color:blue;margin-top:35px;margin-bottom:30px;">8) Encoder, Decoder & Cross-Attention (where self-attention lives)</h3>
<center><hr width="50%" class="solid"></center>

<ul>
  <li><b>Encoder block (BERT-style):</b> self-attention (bidirectional) + feed-forward, with residual connections and <span class="highlight"><a href="https://en.wikipedia.org/wiki/Layer_normalization" target="_blank">LayerNorm</a></span>.</li>
  <li><b>Decoder block (GPT-style):</b> masked self-attention (causal) + feed-forward. It looks only leftward in time.</li>
  <li><b>Cross-attention (encoder-decoder, e.g., translation):</b> decoder queries attend over encoder keys/values: \(Q\) from decoder, \(K,V\) from encoder.</li>
</ul>

<div class="image-container">
  <img src="Encoder-Decoder-Attention.png" alt="Encoder self-attention, decoder masked self-attention, and cross-attention" class="responsive-image">
</div>

<!-- ========== 9. COMPLEXITY ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="complexity" style="color:blue;margin-top:35px;margin-bottom:30px;">9) Complexity & efficiency</h3>
<center><hr width="50%" class="solid"></center>

<p>Computing \(QK^\top\) is \(O(n^2 d_k)\) time and \(O(n^2)\) memory for the attention matrix \(A\). That’s why long contexts are expensive. Practical tricks include:</p>
<ul class="checklist">
  <li><b>Chunking/windowed attention:</b> restrict attention to local blocks.</li>
  <li><b>Linear/projection methods:</b> approximate the softmax kernel to get near-linear time.</li>
  <li><b>Memory-efficient kernels:</b> fuse operations and avoid storing full \(A\).</li>
</ul>

<p>For beginners: start with the standard implementation; move to efficient variants only when you hit memory/time limits.</p>

<!-- ========== 10. NUMPY ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="numpy" style="color:blue;margin-top:35px;margin-bottom:30px;">10) NumPy: self-attention in ~30 lines (with optional mask)</h3>
<center><hr width="50%" class="solid"></center>

<pre><code class="language-python">import numpy as np

def softmax(x, axis=-1):
    x = x - x.max(axis=axis, keepdims=True)
    e = np.exp(x)
    return e / e.sum(axis=axis, keepdims=True)

def self_attention_numpy(X, Wq, Wk, Wv, mask=None):
    """
    X:  [n, d_model]
    W*: [d_model, d_k]
    mask: [n, n] with 0 for allowed, -inf for blocked (added before softmax)
    """
    Q = X @ Wq   # [n, d_k]
    K = X @ Wk   # [n, d_k]
    V = X @ Wv   # [n, d_k] (or d_v)

    scores = (Q @ K.T) / np.sqrt(K.shape[1])  # [n, n]
    if mask is not None:
        scores = scores + mask                # add -inf to illegal connections

    A = softmax(scores, axis=-1)              # attention weights
    Y = A @ V                                 # [n, d_k]
    return Y, A

# Example
np.random.seed(0)
n, d_model, d_k = 5, 16, 8
X  = np.random.randn(n, d_model)
Wq = np.random.randn(d_model, d_k)
Wk = np.random.randn(d_model, d_k)
Wv = np.random.randn(d_model, d_k)

# causal mask (upper-triangular set to -inf)
mask = np.triu(np.ones((n,n)) * -1e9, k=1)
Y, A = self_attention_numpy(X, Wq, Wk, Wv, mask=mask)
print("Output shape:", Y.shape, "  Attn shape:", A.shape)</code></pre>

<p class="mini">Notice the mask shape matches the \(n\times n\) attention score matrix.</p>

<!-- ========== 11. PYTORCH ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="pytorch" style="color:blue;margin-top:35px;margin-bottom:30px;">11) PyTorch: from scratch and with <code>MultiheadAttention</code></h3>
<center><hr width="50%" class="solid"></center>

<pre><code class="language-python">import torch, torch.nn as nn, math
torch.manual_seed(0)

def scaled_dot_product_attention(Q, K, V, attn_mask=None):
    # Q,K,V: [B, h, n, d_k]
    scores = Q @ K.transpose(-2, -1) / math.sqrt(Q.size(-1))  # [B,h,n,n]
    if attn_mask is not None:
        scores = scores + attn_mask                           # add -inf where blocked
    A = torch.softmax(scores, dim=-1)
    Y = A @ V                                                 # [B,h,n,d_k]
    return Y, A

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, d_model=128, num_heads=4):
        super().__init__()
        assert d_model % num_heads == 0
        self.h = num_heads
        self.d_k = d_model // num_heads
        self.Wq = nn.Linear(d_model, d_model, bias=False)
        self.Wk = nn.Linear(d_model, d_model, bias=False)
        self.Wv = nn.Linear(d_model, d_model, bias=False)
        self.Wo = nn.Linear(d_model, d_model, bias=False)

    def forward(self, X, attn_mask=None):
        B, n, d_model = X.shape
        Q = self.Wq(X).view(B, n, self.h, self.d_k).transpose(1, 2)  # [B,h,n,d_k]
        K = self.Wk(X).view(B, n, self.h, self.d_k).transpose(1, 2)
        V = self.Wv(X).view(B, n, self.h, self.d_k).transpose(1, 2)

        Y, A = scaled_dot_product_attention(Q, K, V, attn_mask=attn_mask)  # [B,h,n,d_k],[B,h,n,n]
        Y = Y.transpose(1, 2).contiguous().view(B, n, self.h*self.d_k)     # concat heads
        out = self.Wo(Y)                                                   # [B,n,d_model]
        return out, A

# Example usage
B, n, d_model, h = 2, 6, 128, 4
X = torch.randn(B, n, d_model)

# causal mask: [1,1,n,n] broadcastable to [B,h,n,n]
mask = torch.triu(torch.ones(n, n)*-1e9, diagonal=1).unsqueeze(0).unsqueeze(0)
mha = MultiHeadSelfAttention(d_model=d_model, num_heads=h)
Y, A = mha(X, attn_mask=mask)
print(Y.shape, A.shape)  # torch.Size([2, 6, 128]) torch.Size([2, 4, 6, 6])</code></pre>

<p><b>Built-in layer.</b> PyTorch also offers <code>nn.MultiheadAttention</code> (note its default input shape is <code>[seq_len, batch, embed_dim]</code>):</p>

<pre><code class="language-python">mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=h, batch_first=True)
# For self-attention, Q=K=V=X
causal_mask = torch.triu(torch.ones(n, n)*float('-inf'), diagonal=1)  # [n,n]
Y, A = mha(X, X, X, attn_mask=causal_mask)  # Y: [B,n,d_model], A: [B,h,n,n] (as of recent versions)</code></pre>

<div class="tip mini">
<b>Decoder blocks:</b> use the causal mask. <b>Encoder blocks:</b> no causal mask; optionally use a padding mask to hide padded tokens.
</div>

<!-- ========== 12. VISUALS ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="visuals" style="color:blue;margin-top:35px;margin-bottom:30px;">12) Visualizing attention (what heads learn)</h3>
<center><hr width="50%" class="solid"></center>

<p>Plot \(A\) as a heatmap: rows = query positions, columns = key positions. Different heads often specialize:</p>
<ul>
  <li>Short-range syntax (e.g., adjective → noun)</li>
  <li>Long-range dependencies (e.g., subject ↔ verb)</li>
  <li>Coreference (e.g., “it” ↔ its referent)</li>
</ul>

<div class="image-container">
  <img src="Heatmap-Attention.png" alt="Attention heatmap visualization" class="responsive-image">
</div>

<p class="mini">Not every head is interpretable; that’s normal. Use these plots for debugging intuition, not as a definitive explanation.</p>

<!-- ========== 13. GLOSSARY ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="glossary" style="color:blue;margin-top:35px;margin-bottom:30px;">13) Mini-Glossary</h3>
<center><hr width="50%" class="solid"></center>

<ul>
  <li><b>Self-attention</b> — attention where \(Q,K,V\) all come from the same sequence.</li>
  <li><b>Cross-attention</b> — \(Q\) from the decoder, \(K,V\) from the encoder.</li>
  <li><b>Scaled dot-product</b> — use \(\frac{QK^\top}{\sqrt{d_k}}\) before softmax to stabilize training.</li>
  <li><b>Head</b> — one attention subspace with its own \(W_Q,W_K,W_V\).</li>
  <li><b>Positional encoding</b> — inject order info (sinusoidal or learned).</li>
  <li><b>Causal mask</b> — prevents looking at future tokens in generation.</li>
</ul>

<!-- ========== 14. REFERENCES ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="refs" style="color:blue;margin-top:35px;margin-bottom:30px;">14) References & Further Reading</h3>
<center><hr width="50%" class="solid"></center>

<ol>
  <li>Vaswani, A. et al. (2017). <em>Attention Is All You Need</em>. NIPS. (<a href="https://en.wikipedia.org/wiki/Attention_Is_All_You_Need" target="_blank">Transformer overview</a>)</li>
  <li>Jay Alammar. <em>The Illustrated Transformer</em>. (Great visual blog)</li>
  <li>Bengio, Goodfellow, Courville. <em>Deep Learning</em> — sequence models & attention.</li>
  <li>Wikipedia: <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)" target="_blank">Attention (ML)</a>, <a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank">Softmax</a>, <a href="https://en.wikipedia.org/wiki/Layer_normalization" target="_blank">LayerNorm</a>, <a href="https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6" target="_blank">Positional encoding</a>.</li>
</ol>

<!-- ========== 15. VIDEO ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="video" style="color:blue;margin-top:35px;margin-bottom:30px;">15) Video: Karpathy — “Let’s build GPT from scratch”</h3>
<center><hr width="50%" class="solid"></center>

<div class="video-container" style="width:95%;margin:0 auto;">
  <iframe src="https://www.youtube.com/embed/kCc8FmEb1nY"
          title="Andrej Karpathy: Let's build GPT from scratch"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen></iframe>
</div>
<br/>

<!-- ========== 16. FAQ ========== -->
<center><hr width="50%" class="solid"></center>
<h3 id="faq" style="color:blue;margin-top:35px;margin-bottom:30px;">16) FAQ</h3>
<center><hr width="50%" class="solid"></center>

<h4>Q1. Why the \(\sqrt{d_k}\) scaling?</h4>
<p>Dot products grow with \(d_k\). Scaling keeps logits in a good range so softmax isn’t overly peaky, stabilizing gradients.</p>

<h4>Q2. What’s the difference between encoder self-attention and decoder self-attention?</h4>
<p>Encoders attend bidirectionally (see all tokens). Decoders use a <b>causal mask</b> to prevent peeking at future tokens during generation.</p>

<h4>Q3. Do I need multi-head attention?</h4>
<p>Almost always yes. Multiple heads let the model capture different relationships in parallel; single-head is a useful teaching simplification.</p>

<h4>Q4. How do positional encodings get added?</h4>
<p>Typically by addition: \(X_\text{input}=X_\text{embed}+\text{PE}\). Learned or sinusoidal both work; sinusoidal gives nice extrapolation properties.</p>

<h4>Q5. Why are long contexts expensive?</h4>
<p>The attention matrix is \(n\times n\). Both compute and memory grow quadratically with sequence length.</p>

      </div> <!-- /postBody-div -->
    </div> <!-- /blogPost-div -->

    <!-- ---------- FOOTER ---------- -->
    <footer class="footer">
      <div class="waves">
        <div class="wave" id="wave1"></div>
        <div class="wave" id="wave2"></div>
        <div class="wave" id="wave3"></div>
      </div>
      <div class="footer-elem-container">
        <ul class="social-icon">
          <li class="social-icon__item"><a class="social-icon__link" href="https://www.linkedin.com/in/hamoutni/" target="_blank"><ion-icon name="logo-linkedIn"></ion-icon></a></li>
          <li class="social-icon__item"><a class="social-icon__link" href="https://www.kaggle.com/AnasHamoutni" target="_blank"><i class="fab fa-kaggle foote"></i></a></li>
          <li class="social-icon__item"><a class="social-icon__link" href="https://github.com/AnasHamoutni" target="_blank"><ion-icon name="logo-github"></ion-icon></a></li>
          <li class="social-icon__item"><a class="social-icon__link" href="https://x.com/AnasHamoutni" target="_blank">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 462.799" width="32" height="32" style="vertical-align:baseline;"><path fill="#fff" fill-rule="nonzero" d="M403.229 0h78.506L310.219 196.04 512 462.799H354.002L230.261 301.007 88.669 462.799h-78.56l183.455-209.683L0 0h161.999l111.856 147.88L403.229 0zm-27.556 415.805h43.505L138.363 44.527h-46.68l283.99 371.278z"/></svg>
          </a></li>
        </ul>
    <ul class="menu">
      <li class="menu__item"><a class="menu__link" href="/index.html">Home</a></li>
      <li class="menu__item"><a class="menu__link" href="/Projects.html">Projects</a></li>
      <li class="menu__item"><a class="menu__link" href="/About-me.html">About</a></li>
      <li class="menu__item"><a class="menu__link" href="/Blog.html">Blog</a></li>
      <li class="menu__item"><a class="menu__link" href="/Get-in-Touch.html">Get in Touch</a></li>
      <li class="menu__item"><a class="menu__link" href="/Privacy-Policy.html">Privacy Policy</a></li>
      <li class="menu__item"><a class="menu__link" href="/Terms-of-Service.html">Terms of Service</a></li>
      <li class="menu__item"><a class="menu__link" href="/Cookie-Policy.html">Cookie Policy</a></li>
    </ul>
<div class="p-foot-div">
    <p>&copy;<span class="js-copyright-year">2025</span> Kudos AI • by Anas HAMOUTNI • All Rights Reserved</p>
</div>
</div>
    </footer>

    <!-- ---------- SCRIPTS ---------- -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script>
    <script nomodule src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.js"></script>
    <script src='https://kit.fontawesome.com/d25a67fa19.js' crossorigin='anonymous' defer></script>
    <script src="/index.js" defer></script>
  </body>
</html>
